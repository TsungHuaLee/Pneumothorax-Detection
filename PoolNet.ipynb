{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os, shutil, glob, sys, math, cv2\n",
    "import pydicom\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import albumentations as albu\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as cp\n",
    "from collections import OrderedDict\n",
    "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "from torch import Tensor\n",
    "from torch.jit.annotations import List\n",
    "from torchsummary import summary\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dicom_folder = '/work/tsung1271232/ncku-dataset/train/dicom'\n",
    "train_label_folder = '/work/tsung1271232/train/label'\n",
    "valid_dicom_folder = '/work/tsung1271232/ncku-dataset/test/dicom'\n",
    "valid_label_folder = '/work/tsung1271232/ncku-dataset/test/label'\n",
    "normal_folder = '/work/tsung1271232/ncku-dataset/normal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1656 1656 415 415\n"
     ]
    }
   ],
   "source": [
    "train_dicom_fp = []\n",
    "train_label_fp = []\n",
    "valid_dicom_fp = []\n",
    "valid_label_fp = []\n",
    "for i in sorted(os.listdir(train_dicom_folder)):\n",
    "    name, extension = os.path.splitext(i)\n",
    "    train_dicom_fp.append(os.path.join(train_dicom_folder, i))\n",
    "    if os.path.exists(os.path.join(train_label_folder, name, 'label.png')):\n",
    "        train_label_fp.append(os.path.join(train_label_folder, name, 'label.png'))\n",
    "    else:\n",
    "        train_label_fp.append(None)\n",
    "        \n",
    "for i in sorted(os.listdir(valid_dicom_folder)):\n",
    "    name, extension = os.path.splitext(i)\n",
    "    valid_dicom_fp.append(os.path.join(valid_dicom_folder, i))\n",
    "    if os.path.exists(os.path.join(valid_label_folder, name, 'label.png')):\n",
    "        valid_label_fp.append(os.path.join(valid_label_folder, name, 'label.png'))\n",
    "    else:\n",
    "        valid_label_fp.append(None)\n",
    "print(len(train_dicom_fp), len(train_label_fp), len(valid_dicom_fp), len(valid_label_fp) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1340 661\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "normal = sorted(os.listdir(normal_folder))\n",
    "train_normal, valid_normal = train_test_split(normal, test_size=0.33, random_state=42)\n",
    "print(len(train_normal), len(valid_normal))\n",
    "for i in train_normal:\n",
    "    train_dicom_fp.append(os.path.join(normal_folder, i))\n",
    "    train_label_fp.append(None)\n",
    "\n",
    "for i in valid_normal:\n",
    "    valid_dicom_fp.append(os.path.join(normal_folder, i))\n",
    "    valid_label_fp.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2996 729\n",
      "2996 2996 1076 1076\n"
     ]
    }
   ],
   "source": [
    "train_dicom_fp = np.array(train_dicom_fp)\n",
    "train_label_fp = np.array(train_label_fp)\n",
    "valid_dicom_fp = np.array(valid_dicom_fp)\n",
    "valid_label_fp = np.array(valid_label_fp)\n",
    "print(len(np.where(train_label_fp == None)[0]), len(np.where(valid_label_fp == None)[0]))\n",
    "print(len(train_dicom_fp), len(train_label_fp), len(valid_dicom_fp), len(valid_label_fp) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "\n",
    "def image_mask_preprocessing(image, mask, height = 512, width = 512, **kwargs):\n",
    "    larger_side = max(image.shape[0], image.shape[1])\n",
    "    \n",
    "    aug = albu.Compose([\n",
    "        albu.PadIfNeeded(min_height=larger_side, min_width=larger_side, always_apply=True, border_mode=0),\n",
    "        albu.Resize(height=height, width=width , always_apply=True,)\n",
    "    ])\n",
    "    \n",
    "    sample = aug(image=image, mask=mask)\n",
    "    image, mask = sample['image'], sample['mask']\n",
    "    \n",
    "    # normalize\n",
    "    if 'is_norm' in kwargs and kwargs['is_norm'] == False:\n",
    "        pass\n",
    "    else:\n",
    "        image = (image - image.min()) / (image.max() - image.min()) * (255 - 0) + 0\n",
    "        image = image.astype('uint8')\n",
    "    \n",
    "    mask = np.where(mask > 0, 1, 0)\n",
    "    # convert to 3 channel\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            images_fps, \n",
    "            masks_fps, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        self.images_fps = images_fps\n",
    "        self.masks_fps = masks_fps\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # dicom\n",
    "        dcm = pydicom.dcmread(self.images_fps[i])\n",
    "        image = dcm.pixel_array\n",
    "        \n",
    "        if(self.masks_fps[i] == None):\n",
    "            mask = np.zeros_like(image)\n",
    "        else:\n",
    "            mask = cv2.imread(self.masks_fps[i], cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        image, mask = image_mask_preprocessing(image, mask, **self.kwargs)\n",
    "        mask = np.expand_dims(mask, axis=-1).astype('float')\n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images_fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encoder/resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BasicBlock(nn.Module):\n",
    "#     expansion = 1\n",
    "\n",
    "#     def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "#                  base_width=64, dilation=1, norm_layer=None):\n",
    "#         super(BasicBlock, self).__init__()\n",
    "#         if norm_layer is None:\n",
    "#             norm_layer = nn.BatchNorm2d\n",
    "#         if groups != 1 or base_width != 64:\n",
    "#             raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "#         if dilation > 1:\n",
    "#             raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "#         # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "#         self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "#         self.bn1 = norm_layer(planes)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.conv2 = conv3x3(planes, planes)\n",
    "#         self.bn2 = norm_layer(planes)\n",
    "#         self.downsample = downsample\n",
    "#         self.stride = stride\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         identity = x\n",
    "\n",
    "#         out = self.conv1(x)\n",
    "#         out = self.bn1(out)\n",
    "#         out = self.relu(out)\n",
    "\n",
    "#         out = self.conv2(out)\n",
    "#         out = self.bn2(out)\n",
    "\n",
    "#         if self.downsample is not None:\n",
    "#             identity = self.downsample(x)\n",
    "\n",
    "#         out += identity\n",
    "#         out = self.relu(out)\n",
    "\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# from torchvision.models.resnet import ResNet\n",
    "# from torchvision.models.resnet import Bottleneck\n",
    "# from pretrainedmodels.models.torchvision_models import pretrained_settings\n",
    "\n",
    "# from segmentation_models_pytorch.encoders._base import EncoderMixin\n",
    "\n",
    "# class ResNetEncoder(ResNet, EncoderMixin):\n",
    "#     def __init__(self, out_channels, depth=5, **kwargs):\n",
    "#         super().__init__(**kwargs)\n",
    "#         self._depth = depth\n",
    "#         self._out_channels = out_channels\n",
    "#         self._in_channels = 3\n",
    "\n",
    "#         del self.fc\n",
    "#         del self.avgpool\n",
    "\n",
    "#     def get_stages(self):\n",
    "#         return [\n",
    "#             nn.Identity(),\n",
    "#             nn.Sequential(self.conv1, self.bn1, self.relu),\n",
    "#             nn.Sequential(self.maxpool, self.layer1),\n",
    "#             self.layer2,\n",
    "#             self.layer3,\n",
    "#             self.layer4,\n",
    "#         ]\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         stages = self.get_stages()\n",
    "\n",
    "#         features = []\n",
    "#         for i in range(self._depth + 1):\n",
    "#             x = stages[i](x)\n",
    "#             features.append(x)\n",
    "\n",
    "#         return features\n",
    "\n",
    "#     def load_state_dict(self, state_dict, **kwargs):\n",
    "#         state_dict.pop(\"fc.bias\")\n",
    "#         state_dict.pop(\"fc.weight\")\n",
    "#         super().load_state_dict(state_dict, **kwargs)\n",
    "\n",
    "\n",
    "# resnet_encoders = {\n",
    "#     \"resnet34\": {\n",
    "#         \"encoder\": ResNetEncoder,\n",
    "#         \"pretrained_settings\": pretrained_settings[\"resnet34\"],\n",
    "#         \"params\": {\n",
    "#             \"out_channels\": (3, 64, 64, 128, 256, 512),\n",
    "#             \"block\": BasicBlock,\n",
    "#             \"layers\": [3, 4, 6, 3],\n",
    "#         },\n",
    "#     },\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from segmentation_models_pytorch.base import modules\n",
    "\n",
    "class PSPBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, pool_size, use_bathcnorm=True):\n",
    "        super().__init__()\n",
    "        if pool_size == 1:\n",
    "            use_bathcnorm = False  # PyTorch does not support BatchNorm for 1x1 shape\n",
    "        self.pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(output_size=(pool_size, pool_size)),\n",
    "            modules.Conv2dReLU(in_channels, out_channels, (1, 1), use_batchnorm=use_bathcnorm)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.size(2), x.size(3)\n",
    "        x = self.pool(x)\n",
    "        x = F.interpolate(x, size=(h, w), mode='bilinear', align_corners=True)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PSPModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, sizes=(1, 2, 3, 6), use_bathcnorm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            PSPBlock(in_channels, in_channels // len(sizes), size, use_bathcnorm=use_bathcnorm) for size in sizes\n",
    "        ])\n",
    "        self.conv = nn.Conv2d(in_channels * 2, out_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        xs = [block(x) for block in self.blocks] + [x]\n",
    "        x = torch.cat(xs, dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FAMBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, pool_size, use_bathcnorm=True):\n",
    "        super().__init__()\n",
    "        if pool_size == 1:\n",
    "            use_bathcnorm = False  # PyTorch does not support BatchNorm for 1x1 shape\n",
    "        self.pool = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=(pool_size, pool_size), stride=(pool_size, pool_size)),\n",
    "            modules.Conv2dReLU(in_channels, out_channels, (1, 1), use_batchnorm=use_bathcnorm)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.size(2), x.size(3)\n",
    "        x = self.pool(x)\n",
    "        x = F.interpolate(x, size=(h, w), mode='bilinear', align_corners=True)\n",
    "        return x\n",
    "\n",
    "class FAMModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channel, sizes=(1, 2, 4, 8), use_bathcnorm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            FAMBlock(in_channels, in_channels // len(sizes), size, use_bathcnorm=use_bathcnorm) for size in sizes\n",
    "        ])\n",
    "\n",
    "        self.conv = modules.Conv2dReLU(in_channels // len(sizes), out_channel, (3, 3), 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = [block(x) for block in self.blocks] + [x]\n",
    "        x_add = torch.add(xs[0], xs[1])\n",
    "        x_add = torch.add(x_add, xs[2])\n",
    "        x_add = torch.add(x_add, xs[3])\n",
    "        \n",
    "        conv_out = self.conv(x_add)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Conv3x3GNReLU(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, upsample=False):\n",
    "        super().__init__()\n",
    "        self.upsample = upsample\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, (3, 3), stride=1, padding=1, bias=False\n",
    "            ),\n",
    "            nn.GroupNorm(32, out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "        if self.upsample:\n",
    "            x = F.interpolate(x, scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FPNBlock(nn.Module):\n",
    "    def __init__(self, pyramid_channels, skip_channels):\n",
    "        super().__init__()\n",
    "        self.skip_conv = nn.Conv2d(skip_channels, pyramid_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x, skip=None, ggf=None):\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        skip = self.skip_conv(skip)\n",
    "        x = x + skip + ggf\n",
    "        return x\n",
    "\n",
    "\n",
    "class SegmentationBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_upsamples=0):\n",
    "        super().__init__()\n",
    "\n",
    "        blocks = [Conv3x3GNReLU(in_channels, out_channels, upsample=bool(n_upsamples))]\n",
    "\n",
    "        if n_upsamples > 1:\n",
    "            for _ in range(1, n_upsamples):\n",
    "                blocks.append(Conv3x3GNReLU(out_channels, out_channels, upsample=True))\n",
    "\n",
    "        self.block = nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class MergeBlock(nn.Module):\n",
    "    def __init__(self, policy):\n",
    "        super().__init__()\n",
    "        if policy not in [\"add\", \"cat\"]:\n",
    "            raise ValueError(\n",
    "                \"`merge_policy` must be one of: ['add', 'cat'], got {}\".format(\n",
    "                    policy\n",
    "                )\n",
    "            )\n",
    "        self.policy = policy\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.policy == 'add':\n",
    "            return sum(x)\n",
    "        elif self.policy == 'cat':\n",
    "            return torch.cat(x, dim=1)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"`merge_policy` must be one of: ['add', 'cat'], got {}\".format(self.policy)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPNDecoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder_channels,\n",
    "            encoder_depth=5,\n",
    "            pyramid_channels=256,\n",
    "            segmentation_channels=128,\n",
    "            dropout=0.2,\n",
    "            merge_policy=\"add\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.out_channels = segmentation_channels if merge_policy == \"add\" else segmentation_channels * 4\n",
    "        if encoder_depth < 3:\n",
    "            raise ValueError(\"Encoder depth for FPN decoder cannot be less than 3, got {}.\".format(encoder_depth))\n",
    "\n",
    "        encoder_channels = encoder_channels[::-1]\n",
    "        encoder_channels = encoder_channels[:encoder_depth + 1]\n",
    "\n",
    "        self.p5 = nn.Conv2d(encoder_channels[0], pyramid_channels, kernel_size=1)\n",
    "        self.p4 = FPNBlock(pyramid_channels, encoder_channels[1])\n",
    "        self.p3 = FPNBlock(pyramid_channels, encoder_channels[2])\n",
    "        self.p2 = FPNBlock(pyramid_channels, encoder_channels[3])\n",
    "\n",
    "        self.seg_blocks = nn.ModuleList([\n",
    "            SegmentationBlock(pyramid_channels, segmentation_channels, n_upsamples=n_upsamples)\n",
    "            for n_upsamples in [3, 2, 1, 0]\n",
    "        ])\n",
    "\n",
    "        self.merge = MergeBlock(merge_policy)\n",
    "        self.dropout = nn.Dropout2d(p=dropout, inplace=True)\n",
    "\n",
    "        self.GGM = PSPModule(encoder_channels[0], pyramid_channels, sizes=(1,2,3,6))\n",
    "        self.f4 = FAMModule(pyramid_channels, pyramid_channels)\n",
    "        self.f3 = FAMModule(pyramid_channels, pyramid_channels)\n",
    "        self.f2 = FAMModule(pyramid_channels, pyramid_channels)\n",
    "\n",
    "    def forward(self, *features):\n",
    "        c2, c3, c4, c5 = features[-4:]\n",
    "\n",
    "        GGM = self.GGM(c5)\n",
    "\n",
    "        p5 = self.p5(c5)\n",
    "\n",
    "        GGF4 =  F.interpolate(GGM, scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        p4 = self.p4(p5, c4, GGF4)\n",
    "        f4 = self.f4(p4)\n",
    "\n",
    "        GGF3 =  F.interpolate(GGF4, scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        p3 = self.p3(f4, c3, GGF3)\n",
    "        f3 = self.f3(p3)\n",
    "\n",
    "        GGF2 =  F.interpolate(GGF3, scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        p2 = self.p2(p3, c2, GGF2)\n",
    "        f2 = self.f3(p2)\n",
    "        \n",
    "        feature_pyramid = [seg_block(p) for seg_block, p in zip(self.seg_blocks, [p5, f4, f3, f2])]\n",
    "\n",
    "        x = self.merge(feature_pyramid)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "from segmentation_models_pytorch.base.heads import SegmentationHead, ClassificationHead\n",
    "from segmentation_models_pytorch.base.model import SegmentationModel\n",
    "from segmentation_models_pytorch.encoders import get_encoder\n",
    "import torch\n",
    "\n",
    "class FPN(SegmentationModel):\n",
    "    def __init__(self,\n",
    "        encoder_name: str = \"resnet34\",\n",
    "        in_channels: int = 3,\n",
    "        encoder_depth: int = 5,\n",
    "        encoder_weights: Optional[str] = \"imagenet\",\n",
    "        decoder_pyramid_channels: int = 256,\n",
    "        decoder_segmentation_channels: int = 128,\n",
    "        decoder_merge_policy: str = \"add\",\n",
    "        decoder_dropout: float = 0.2,\n",
    "        classes: int = 1,\n",
    "        activation: Optional[str] = None,\n",
    "        upsampling: int = 4,\n",
    "        aux_params: Optional[dict] = None\n",
    "    ):\n",
    "        super(FPN, self).__init__()\n",
    "\n",
    "        self.encoder = get_encoder(\n",
    "            encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            depth=encoder_depth,\n",
    "            weights=encoder_weights,\n",
    "        )\n",
    "\n",
    "        self.decoder = FPNDecoder(\n",
    "            encoder_channels=self.encoder.out_channels,\n",
    "            encoder_depth=encoder_depth,\n",
    "            pyramid_channels=decoder_pyramid_channels,\n",
    "            segmentation_channels=decoder_segmentation_channels,\n",
    "            dropout=decoder_dropout,\n",
    "            merge_policy=decoder_merge_policy,\n",
    "        )\n",
    "\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=self.decoder.out_channels,\n",
    "            out_channels=classes,\n",
    "            activation=activation,\n",
    "            kernel_size=1,\n",
    "            upsampling=upsampling,\n",
    "        )\n",
    "\n",
    "        if aux_params is not None:\n",
    "            self.classification_head = ClassificationHead(\n",
    "                in_channels=self.encoder.out_channels[-1], **aux_params\n",
    "            )\n",
    "        else:\n",
    "            self.classification_head = None\n",
    "\n",
    "        self.name = \"fpn-{}\".format(encoder_name)\n",
    "        self.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epoch - cls loss & seg loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm as tqdm\n",
    "from segmentation_models_pytorch.utils.meter import AverageValueMeter\n",
    "class Epoch:\n",
    "    def __init__(self, model, loss, metrics, stage_name, device='cpu', verbose=True):\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        self.metrics = metrics\n",
    "        self.stage_name = stage_name\n",
    "        self.verbose = verbose\n",
    "        self.device = device\n",
    "        self._to_device()\n",
    "    def _to_device(self):\n",
    "        self.model.to(self.device)\n",
    "        for loss in self.loss:\n",
    "            loss.to(self.device)\n",
    "        for metric in self.metrics:\n",
    "            metric.to(self.device)\n",
    "    def _format_logs(self, logs):\n",
    "        str_logs = ['{} - {:.4}'.format(k, v) for k, v in logs.items()]\n",
    "        s = ', '.join(str_logs)\n",
    "        return s\n",
    "    def batch_update(self, x, y, cls):\n",
    "        raise NotImplementedError\n",
    "    def on_epoch_start(self):\n",
    "        pass\n",
    "    def run(self, dataloader):\n",
    "        self.on_epoch_start()\n",
    "        logs = {}\n",
    "        seg_loss_meter = AverageValueMeter()\n",
    "        cls_loss_meter = AverageValueMeter()\n",
    "        seg_metrics_meters = AverageValueMeter()\n",
    "        cls_metrics_meters = AverageValueMeter()\n",
    "        with tqdm(dataloader, desc=self.stage_name, file=sys.stdout, disable=not (self.verbose)) as iterator:\n",
    "            for x, y, cls in iterator:\n",
    "                x, y, cls = x.to(self.device), y.to(self.device), cls.to(self.device)\n",
    "                seg_loss, cls_loss, seg_pred, cls_pred = self.batch_update(x, y, cls)\n",
    "                # update loss logs\n",
    "                seg_loss_value = seg_loss.cpu().detach().numpy()\n",
    "                seg_loss_meter.add(seg_loss_value)\n",
    "                cls_loss_value = cls_loss.cpu().detach().numpy()\n",
    "                cls_loss_meter.add(cls_loss_value)\n",
    "                loss_logs = {self.loss[0].__name__: seg_loss_meter.mean, 'cls_bce_loss': cls_loss_meter.mean}\n",
    "                logs.update(loss_logs)\n",
    "                # update metrics logs\n",
    "                for metric_fn in self.metrics:\n",
    "                    seg_metric_value = metric_fn(seg_pred, y).cpu().detach().numpy()\n",
    "                    seg_metrics_meters.add(seg_metric_value)\n",
    "                _, cls_pred = torch.max(cls_pred.data, 1)\n",
    "                cls_metric_value = (cls_pred == cls).sum().float() / cls.shape[0]\n",
    "                cls_metric_value = cls_metric_value.detach().cpu().numpy()\n",
    "                cls_metrics_meters.add(cls_metric_value)\n",
    "                seg_metrics_logs = {'dice' : seg_metrics_meters.mean}\n",
    "                cls_metrics_logs = {'acc' : cls_metrics_meters.mean}\n",
    "                logs.update(seg_metrics_logs)\n",
    "                logs.update(cls_metrics_logs)\n",
    "                if self.verbose:\n",
    "                    s = self._format_logs(logs)\n",
    "                    iterator.set_postfix_str(s)\n",
    "        return logs\n",
    "    \n",
    "class TrainEpoch(Epoch):\n",
    "    def __init__(self, model, loss, metrics, optimizer, device='cpu', verbose=True):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            loss=loss,\n",
    "            metrics=metrics,\n",
    "            stage_name='train',\n",
    "            device=device,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.optimizer = optimizer\n",
    "    def on_epoch_start(self):\n",
    "        self.model.train()\n",
    "    def batch_update(self, x, y, cls):\n",
    "        self.optimizer.zero_grad()\n",
    "        seg_prediction, cls_prediction = self.model.forward(x)\n",
    "        seg_loss = self.loss[0](seg_prediction, y)\n",
    "        cls_loss = self.loss[1](cls_prediction, cls)\n",
    "        loss = seg_loss + cls_loss\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return seg_loss, cls_loss, seg_prediction, cls_prediction\n",
    "    \n",
    "class ValidEpoch(Epoch):\n",
    "    def __init__(self, model, loss, metrics, device='cpu', verbose=True):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            loss=loss,\n",
    "            metrics=metrics,\n",
    "            stage_name='valid',\n",
    "            device=device,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "    def on_epoch_start(self):\n",
    "        self.model.eval()\n",
    "    def batch_update(self, x, y, cls):\n",
    "        with torch.no_grad():\n",
    "            seg_prediction, cls_prediction = self.model.forward(x)\n",
    "            seg_loss = self.loss[0](seg_prediction, y)\n",
    "            cls_loss = self.loss[1](cls_prediction, cls)\n",
    "        return seg_loss, cls_loss, seg_prediction, cls_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "\n",
    "        albu.RandomCrop(height = 768, width = 768, always_apply = True),\n",
    "        \n",
    "        albu.ShiftScaleRotate(scale_limit=0, rotate_limit=10, shift_limit=0.1, p=1, border_mode=0),\n",
    "\n",
    "        albu.IAAAdditiveGaussianNoise(p=0.2),\n",
    "        albu.IAAPerspective(p=0.5),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.CLAHE(p=1),\n",
    "                albu.RandomBrightness(p=1),\n",
    "                albu.RandomGamma(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.IAASharpen(p=1),\n",
    "                albu.Blur(blur_limit=3, p=1),\n",
    "                albu.MotionBlur(blur_limit=3, p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.RandomContrast(p=1),\n",
    "                albu.HueSaturationValue(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing():\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "#         albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPN(\n",
      "  (encoder): ResNetEncoder(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (5): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      "  (decoder): FPNDecoder(\n",
      "    (p5): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (p4): FPNBlock(\n",
      "      (skip_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (p3): FPNBlock(\n",
      "      (skip_conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (p2): FPNBlock(\n",
      "      (skip_conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (seg_blocks): ModuleList(\n",
      "      (0): SegmentationBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv3x3GNReLU(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "              (2): ReLU(inplace)\n",
      "            )\n",
      "          )\n",
      "          (1): Conv3x3GNReLU(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "              (2): ReLU(inplace)\n",
      "            )\n",
      "          )\n",
      "          (2): Conv3x3GNReLU(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "              (2): ReLU(inplace)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): SegmentationBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv3x3GNReLU(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "              (2): ReLU(inplace)\n",
      "            )\n",
      "          )\n",
      "          (1): Conv3x3GNReLU(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "              (2): ReLU(inplace)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): SegmentationBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv3x3GNReLU(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "              (2): ReLU(inplace)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): SegmentationBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv3x3GNReLU(\n",
      "            (block): Sequential(\n",
      "              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "              (2): ReLU(inplace)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (merge): MergeBlock()\n",
      "    (dropout): Dropout2d(p=0.2, inplace)\n",
      "    (GGM): PSPModule(\n",
      "      (blocks): ModuleList(\n",
      "        (0): PSPBlock(\n",
      "          (pool): Sequential(\n",
      "            (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (1): Conv2dReLU(\n",
      "              (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (1): Identity()\n",
      "              (2): ReLU(inplace)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): PSPBlock(\n",
      "          (pool): Sequential(\n",
      "            (0): AdaptiveAvgPool2d(output_size=(2, 2))\n",
      "            (1): Conv2dReLU(\n",
      "              (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): PSPBlock(\n",
      "          (pool): Sequential(\n",
      "            (0): AdaptiveAvgPool2d(output_size=(3, 3))\n",
      "            (1): Conv2dReLU(\n",
      "              (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): PSPBlock(\n",
      "          (pool): Sequential(\n",
      "            (0): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "            (1): Conv2dReLU(\n",
      "              (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (f4): FAMModule(\n",
      "      (blocks): ModuleList(\n",
      "        (0): FAMBlock(\n",
      "          (pool): Sequential(\n",
      "            (0): AvgPool2d(kernel_size=(1, 1), stride=(1, 1), padding=0)\n",
      "            (1): Conv2dReLU(\n",
      "              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (1): Identity()\n",
      "              (2): ReLU(inplace)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): FAMBlock(\n",
      "          (pool): Sequential(\n",
      "            (0): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
      "            (1): Conv2dReLU(\n",
      "              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): FAMBlock(\n",
      "          (pool): Sequential(\n",
      "            (0): AvgPool2d(kernel_size=(4, 4), stride=(4, 4), padding=0)\n",
      "            (1): Conv2dReLU(\n",
      "              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): FAMBlock(\n",
      "          (pool): Sequential(\n",
      "            (0): AvgPool2d(kernel_size=(8, 8), stride=(8, 8), padding=0)\n",
      "            (1): Conv2dReLU(\n",
      "              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv2dReLU(\n",
      "        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (f3): FAMModule(\n",
      "      (blocks): ModuleList(\n",
      "        (0): FAMBlock(\n",
      "          (pool): Sequential(\n",
      "            (0): AvgPool2d(kernel_size=(1, 1), stride=(1, 1), padding=0)\n",
      "            (1): Conv2dReLU(\n",
      "              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (1): Identity()\n",
      "              (2): ReLU(inplace)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): FAMBlock(\n",
      "          (pool): Sequential(\n",
      "            (0): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
      "            (1): Conv2dReLU(\n",
      "              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): FAMBlock(\n",
      "          (pool): Sequential(\n",
      "            (0): AvgPool2d(kernel_size=(4, 4), stride=(4, 4), padding=0)\n",
      "            (1): Conv2dReLU(\n",
      "              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): FAMBlock(\n",
      "          (pool): Sequential(\n",
      "            (0): AvgPool2d(kernel_size=(8, 8), stride=(8, 8), padding=0)\n",
      "            (1): Conv2dReLU(\n",
      "              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv2dReLU(\n",
      "        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (f2): FAMModule(\n",
      "      (blocks): ModuleList(\n",
      "        (0): FAMBlock(\n",
      "          (pool): Sequential(\n",
      "            (0): AvgPool2d(kernel_size=(1, 1), stride=(1, 1), padding=0)\n",
      "            (1): Conv2dReLU(\n",
      "              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (1): Identity()\n",
      "              (2): ReLU(inplace)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): FAMBlock(\n",
      "          (pool): Sequential(\n",
      "            (0): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
      "            (1): Conv2dReLU(\n",
      "              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): FAMBlock(\n",
      "          (pool): Sequential(\n",
      "            (0): AvgPool2d(kernel_size=(4, 4), stride=(4, 4), padding=0)\n",
      "            (1): Conv2dReLU(\n",
      "              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): FAMBlock(\n",
      "          (pool): Sequential(\n",
      "            (0): AvgPool2d(kernel_size=(8, 8), stride=(8, 8), padding=0)\n",
      "            (1): Conv2dReLU(\n",
      "              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (2): ReLU(inplace)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv): Conv2dReLU(\n",
      "        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (segmentation_head): SegmentationHead(\n",
      "    (0): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): UpsamplingBilinear2d(scale_factor=4.0, mode=bilinear)\n",
      "    (2): Activation(\n",
      "      (activation): Identity()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ENCODER = 'resnet34'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = ['penu']\n",
    "ACTIVATION = None # could be None for logits or 'softmax2d' for multicalss segmentation\n",
    "DEVICE = 'cuda'\n",
    "kwargs = {'classes': 2}\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model = FPN(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    "#     aux_params= kwargs\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24322689\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler\n",
    "epoch = 100\n",
    "batch_size = 32\n",
    "\n",
    "# model settings\n",
    "loss = smp.utils.losses.BCEWithLogitsLoss()\n",
    "# loss = [\n",
    "#     smp.utils.losses.BCEWithLogitsLoss(),\n",
    "#     smp.utils.losses.CrossEntropyLoss()\n",
    "# ]\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(),\n",
    "]\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=1e-4),\n",
    "])\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer = optimizer, T_max=epoch)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "# train_epoch = TrainEpoch(\n",
    "#     model, \n",
    "#     loss=loss, \n",
    "#     metrics=metrics, \n",
    "#     optimizer=optimizer,\n",
    "#     device=DEVICE,\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "# valid_epoch = ValidEpoch(\n",
    "#     model, \n",
    "#     loss=loss, \n",
    "#     metrics=metrics, \n",
    "#     device=DEVICE,\n",
    "#     verbose=True,\n",
    "# )\n",
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data settings\n",
    "train_dataset = Dataset(\n",
    "    train_dicom_fp, \n",
    "    train_label_fp, \n",
    "    augmentation=get_training_augmentation(), \n",
    "    preprocessing=get_preprocessing(),\n",
    "    height = 1024,\n",
    "    width = 1024\n",
    ")\n",
    "valid_dataset = Dataset(\n",
    "    valid_dicom_fp, \n",
    "    valid_label_fp, \n",
    "    preprocessing=get_preprocessing(),\n",
    "    height = 1024,\n",
    "    width = 1024\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=20)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# current_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
    "\n",
    "# model_folder_name = '/home/tsung1271232/pneumothorax-segmentation/weight/'\n",
    "# model_name = model_folder_name + str(current_time) + \"_aug:Packagedefault_PoolNet_bs:{}_PoolNet\".format(batch_size)\n",
    "\n",
    "# max_score = 0\n",
    "\n",
    "# for i in range(0, epoch):\n",
    "#     print('\\nEpoch: {}, batch: {}'.format(i, batch_size))\n",
    "#     train_logs = train_epoch.run(train_loader)\n",
    "#     valid_logs = valid_epoch.run(valid_loader)\n",
    "#     lr_scheduler.step()\n",
    "#     # do something (save model, change lr, etc.)\n",
    "#     if max_score < valid_logs['iou_score']:\n",
    "#         max_score = valid_logs['iou_score']\n",
    "# #         torch.save(model, model_name+\"_epoch:{}-fscore:{:.2f}.pth\".format(i, max_score))\n",
    "#         torch.save(model.state_dict(), model_name+\"_epoch:{}-bce:{:.2f}.pth\".format(i, max_score))\n",
    "#         print('Model saved! {}'.format(model_name+\"_epoch:{}-bce:{:.2f}.pth\".format(i, max_score)))\n",
    "        \n",
    "# #     if i == 35:\n",
    "# #         optimizer.param_groups[0]['lr'] = 5e-5\n",
    "# #         print('Decrease decoder learning rate to 1e-5!')\n",
    "        \n",
    "# #     if i == 75:\n",
    "# #         optimizer.param_groups[0]['lr'] = 5e-6\n",
    "# #         print('Decrease decoder learning rate to 1e-6!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), model_name+\"_epoch:{}-bce:{:.2f}.pth\".format(99, 0.9089))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FPN(\n",
       "  (encoder): ResNetEncoder(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (decoder): FPNDecoder(\n",
       "    (p5): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (p4): FPNBlock(\n",
       "      (skip_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (p3): FPNBlock(\n",
       "      (skip_conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (p2): FPNBlock(\n",
       "      (skip_conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (seg_blocks): ModuleList(\n",
       "      (0): SegmentationBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace)\n",
       "            )\n",
       "          )\n",
       "          (1): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace)\n",
       "            )\n",
       "          )\n",
       "          (2): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): SegmentationBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace)\n",
       "            )\n",
       "          )\n",
       "          (1): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): SegmentationBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): SegmentationBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (merge): MergeBlock()\n",
       "    (dropout): Dropout2d(p=0.2, inplace)\n",
       "    (GGM): PSPModule(\n",
       "      (blocks): ModuleList(\n",
       "        (0): PSPBlock(\n",
       "          (pool): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "            (1): Conv2dReLU(\n",
       "              (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): Identity()\n",
       "              (2): ReLU(inplace)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): PSPBlock(\n",
       "          (pool): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=(2, 2))\n",
       "            (1): Conv2dReLU(\n",
       "              (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): PSPBlock(\n",
       "          (pool): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=(3, 3))\n",
       "            (1): Conv2dReLU(\n",
       "              (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): PSPBlock(\n",
       "          (pool): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "            (1): Conv2dReLU(\n",
       "              (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (f4): FAMModule(\n",
       "      (blocks): ModuleList(\n",
       "        (0): FAMBlock(\n",
       "          (pool): Sequential(\n",
       "            (0): AvgPool2d(kernel_size=(1, 1), stride=(1, 1), padding=0)\n",
       "            (1): Conv2dReLU(\n",
       "              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): Identity()\n",
       "              (2): ReLU(inplace)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): FAMBlock(\n",
       "          (pool): Sequential(\n",
       "            (0): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
       "            (1): Conv2dReLU(\n",
       "              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): FAMBlock(\n",
       "          (pool): Sequential(\n",
       "            (0): AvgPool2d(kernel_size=(4, 4), stride=(4, 4), padding=0)\n",
       "            (1): Conv2dReLU(\n",
       "              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): FAMBlock(\n",
       "          (pool): Sequential(\n",
       "            (0): AvgPool2d(kernel_size=(8, 8), stride=(8, 8), padding=0)\n",
       "            (1): Conv2dReLU(\n",
       "              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv): Conv2dReLU(\n",
       "        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (f3): FAMModule(\n",
       "      (blocks): ModuleList(\n",
       "        (0): FAMBlock(\n",
       "          (pool): Sequential(\n",
       "            (0): AvgPool2d(kernel_size=(1, 1), stride=(1, 1), padding=0)\n",
       "            (1): Conv2dReLU(\n",
       "              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): Identity()\n",
       "              (2): ReLU(inplace)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): FAMBlock(\n",
       "          (pool): Sequential(\n",
       "            (0): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
       "            (1): Conv2dReLU(\n",
       "              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): FAMBlock(\n",
       "          (pool): Sequential(\n",
       "            (0): AvgPool2d(kernel_size=(4, 4), stride=(4, 4), padding=0)\n",
       "            (1): Conv2dReLU(\n",
       "              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): FAMBlock(\n",
       "          (pool): Sequential(\n",
       "            (0): AvgPool2d(kernel_size=(8, 8), stride=(8, 8), padding=0)\n",
       "            (1): Conv2dReLU(\n",
       "              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv): Conv2dReLU(\n",
       "        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "    (f2): FAMModule(\n",
       "      (blocks): ModuleList(\n",
       "        (0): FAMBlock(\n",
       "          (pool): Sequential(\n",
       "            (0): AvgPool2d(kernel_size=(1, 1), stride=(1, 1), padding=0)\n",
       "            (1): Conv2dReLU(\n",
       "              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): Identity()\n",
       "              (2): ReLU(inplace)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): FAMBlock(\n",
       "          (pool): Sequential(\n",
       "            (0): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
       "            (1): Conv2dReLU(\n",
       "              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): FAMBlock(\n",
       "          (pool): Sequential(\n",
       "            (0): AvgPool2d(kernel_size=(4, 4), stride=(4, 4), padding=0)\n",
       "            (1): Conv2dReLU(\n",
       "              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): FAMBlock(\n",
       "          (pool): Sequential(\n",
       "            (0): AvgPool2d(kernel_size=(8, 8), stride=(8, 8), padding=0)\n",
       "            (1): Conv2dReLU(\n",
       "              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv): Conv2dReLU(\n",
       "        (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): UpsamplingBilinear2d(scale_factor=4.0, mode=bilinear)\n",
       "    (2): Activation(\n",
       "      (activation): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FPN(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    "#     aux_params= kwargs\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load('/home/tsung1271232/pneumothorax-segmentation/weight/2020_06_16_12_33_47_aug:Packagedefault_PoolNet_bs:32_PoolNet_epoch:30-iou:0.88.pth'))\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsung1271232/.local/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type FPN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/tsung1271232/.local/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type FPNDecoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/tsung1271232/.local/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type FPNBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/tsung1271232/.local/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type SegmentationBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/tsung1271232/.local/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Conv3x3GNReLU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/tsung1271232/.local/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type MergeBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/tsung1271232/.local/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type PSPModule. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/tsung1271232/.local/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type PSPBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/tsung1271232/.local/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type FAMModule. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/tsung1271232/.local/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type FAMBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, '/home/tsung1271232/Pneumothorax-Detection/stage1_seg.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_logs = valid_epoch.run(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_no_background = LinearSegmentedColormap.from_list(\"\", [\"none\", \"blue\", 'cyan', 'green', 'orange', 'red'])\n",
    "cmap_gt_background2 = LinearSegmentedColormap.from_list(\"\", [\"none\", 'yellow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (data, mask) in enumerate(valid_loader):\n",
    "    pr_mask = model.predict(data.to('cuda'))\n",
    "    act = nn.Sigmoid()\n",
    "    pr_mask = act(pr_mask)\n",
    "    pr_mask = pr_mask.cpu().numpy()\n",
    "    pr_mask[pr_mask < 0.1] = None\n",
    "    \n",
    "    gt_mask = mask.numpy()    \n",
    "    image = data.numpy().astype('uint8')\n",
    "\n",
    "    plt.figure(figsize=(80, 40))\n",
    "    for idx in range(len(gt_mask)):\n",
    "        plt.subplot(4, 8, idx+1).set_title(\"{}-{}\".format(batch_idx, idx))\n",
    "        plt.imshow(image[idx].transpose(1,2,0).reshape(1024, 1024,3), cmap = \"gray\")\n",
    "#         plt.imshow(gt_mask[idx].transpose(1,2,0).reshape(1024, 1024), alpha=0.5, cmap = 'jet')\n",
    "        plt.imshow(pr_mask[idx].transpose(1,2,0).reshape(1024, 1024), alpha=0.5, cmap = 'jet')\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# external dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = '/work/tsung1271232/ncku-dataset/16 cases'\n",
    "# image_folder = '/work/tsung1271232/ncku-dataset/0615testdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_dict = {}\n",
    "for test_image in sorted(os.listdir(image_folder)):\n",
    "    dcm = pydicom.dcmread(os.path.join(image_folder, test_image))\n",
    "    image = dcm.pixel_array\n",
    "    \n",
    "    image, mask = image_mask_preprocessing(image, np.zeros_like(image), 768, 768)\n",
    "    \n",
    "    data = image.transpose(2, 0, 1).astype('float32')\n",
    "    data = np.expand_dims(data, axis = 0)\n",
    "    \n",
    "    data = torch.from_numpy(data)\n",
    "    pr_mask = model.predict(data.to('cuda'))\n",
    "    act = nn.Sigmoid()\n",
    "    pr_mask = act(pr_mask)\n",
    "    pr_mask = pr_mask.cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image.reshape(768, 768,3), cmap = \"gray\")\n",
    "    plt.imshow(pr_mask.squeeze(), alpha=0.5, cmap= 'Reds', vmin=0, vmax=1)\n",
    "    plt.colorbar()\n",
    "    mask_dict[test_image] = pr_mask.squeeze()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/work/tsung1271232/16_cases_masks.pickle', 'wb') as handle:\n",
    "    pickle.dump(mask_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/work/tsung1271232/0615testdata_masks.pickle', 'wb') as handle:\n",
    "    pickle.dump(mask_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = '/work/tsung1271232/siim/train/512_dicom/dicom'\n",
    "for name in os.listdir(train_folder):\n",
    "    image = cv2.imread(os.path.join(train_folder, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_Dataset(BaseDataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            images_fps, \n",
    "            masks_fps, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        self.images_fps = images_fps\n",
    "        self.masks_fps = masks_fps\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # dicom\n",
    "        image = cv2.imread(self.images_fps[i], cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if(self.masks_fps[i] == None):\n",
    "            mask = np.zeros_like(image)\n",
    "        else:\n",
    "            mask = cv2.imread(self.masks_fps[i], cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        image, mask = image_mask_preprocessing(image, mask, **self.kwargs)\n",
    "        mask = np.expand_dims(mask, axis=-1).astype('float')\n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images_fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_fp = []\n",
    "test_mask_fp = []\n",
    "for test_image in sorted(os.listdir('/work/tsung1271232/siim/train/images/1024/image')):\n",
    "    if os.path.exists(os.path.join('/work/tsung1271232/siim/train/images/1024/mask', test_image)):\n",
    "        test_image_fp.append(os.path.join('/work/tsung1271232/siim/train/images/1024/image', test_image))\n",
    "        test_mask_fp.append(os.path.join('/work/tsung1271232/siim/train/images/1024/mask', test_image))\n",
    "\n",
    "test_dataset = test_Dataset(\n",
    "    test_image_fp,\n",
    "    test_mask_fp, \n",
    "    preprocessing=get_preprocessing(),\n",
    "    height = 1024,\n",
    "    width = 1024,\n",
    "    is_norm = False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=20)\n",
    "\n",
    "test_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "# model settings\n",
    "loss = smp.utils.losses.BCEWithLogitsLoss()\n",
    "\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(),\n",
    "]\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=1e-4),\n",
    "])\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_logs = test_epoch.run(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (data, mask) in enumerate(test_loader):\n",
    "    if batch_idx == 5:\n",
    "        pr_mask = model.predict(data.to('cuda'))\n",
    "        act = nn.Sigmoid()\n",
    "        pr_mask = act(pr_mask)\n",
    "        pr_mask = pr_mask.cpu().numpy()\n",
    "        pr_mask[pr_mask < 0.1] = None\n",
    "\n",
    "        gt_mask = mask.numpy()    \n",
    "        image = data.numpy().astype('uint8')\n",
    "\n",
    "        plt.figure(figsize=(80, 40))\n",
    "        for idx in range(len(gt_mask)):\n",
    "            plt.subplot(2, 4, idx+1).set_title(\"{}-{}\".format(batch_idx, idx))\n",
    "            plt.imshow(image[idx].transpose(1,2,0).reshape(1024, 1024,3), cmap = \"gray\")\n",
    "            plt.imshow(gt_mask[idx].transpose(1,2,0).reshape(1024, 1024), alpha=0.5, cmap=ListedColormap(['#ffffff00', 'y']))\n",
    "            plt.imshow(pr_mask[idx].transpose(1,2,0).reshape(1024, 1024), alpha=0.5, cmap=ListedColormap(['#ffffff00', 'm']))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = cv2.imread(test_mask_fp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ttach as tta\n",
    "transforms = tta.Compose(\n",
    "    [\n",
    "        tta.HorizontalFlip(),\n",
    "        tta.Rotate90(angles=[0, 20]),\n",
    "        tta.Scale(scales=[1, 2, 4]),\n",
    "        tta.Multiply(factors=[0.9, 1, 1.1]),        \n",
    "    ]\n",
    ")\n",
    "\n",
    "tta_model = tta.SegmentationTTAWrapper(model, transforms)\n",
    "idx = 0\n",
    "for test_image in sorted(os.listdir('/work/tsung1271232/ncku-dataset/16 cases')):\n",
    "    idx += 1\n",
    "    if(idx != 6):\n",
    "        continue\n",
    "    masks = []\n",
    "    dcm = pydicom.dcmread(os.path.join('/work/tsung1271232/ncku-dataset/16 cases', test_image))\n",
    "    image = dcm.pixel_array\n",
    "\n",
    "    image, mask = image_mask_preprocessing(image, np.zeros_like(image), 1024, 1024)\n",
    "    \n",
    "    data = image.transpose(2, 0, 1).astype('float32')\n",
    "    data = np.expand_dims(data, axis = 0)\n",
    "    \n",
    "    data = torch.from_numpy(data)\n",
    "    \n",
    "    for transformer in transforms: # custom transforms or e.g. tta.aliases.d4_transform() \n",
    "\n",
    "        # augment image\n",
    "        augmented_image = transformer.augment_image(data)\n",
    "        \n",
    "        # pass to model\n",
    "        model_output = model.predict(augmented_image.to('cuda'))\n",
    "        \n",
    "        act = nn.Sigmoid()\n",
    "        model_output = act(model_output)\n",
    "        \n",
    "        # reverse augmentation for mask and label\n",
    "        deaug_mask = transformer.deaugment_mask(model_output)\n",
    "        \n",
    "        # save results\n",
    "        masks.append(deaug_mask)\n",
    "    results = torch.cat(masks, dim=0)\n",
    "    mask = torch.mean(results, dim=0)\n",
    "    # reduce results as you want, e.g mean/max/min\n",
    "    pr_mask = mask.cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.title(test_image)\n",
    "    plt.imshow(image.reshape(1024, 1024,3), cmap = \"gray\")\n",
    "    plt.imshow(pr_mask.squeeze(), alpha=0.5, cmap= 'Reds', vmin=0, vmax=1)\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# segmentation mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data settings\n",
    "train_dataset = Dataset(\n",
    "    train_dicom_fp, \n",
    "    train_label_fp, \n",
    "    preprocessing=get_preprocessing(),\n",
    "    height = 1024,\n",
    "    width = 1024\n",
    ")\n",
    "\n",
    "valid_dataset = Dataset(\n",
    "    valid_dicom_fp, \n",
    "    valid_label_fp, \n",
    "    preprocessing=get_preprocessing(),\n",
    "    height = 1024,\n",
    "    width = 1024\n",
    ")\n",
    "output_folder = '/work/tsung1271232/ncku-dataset/init_seg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx in range(len(train_dataset)):\n",
    "    print(idx, train_dataset.images_fps[idx])\n",
    "    fp = train_dataset.images_fps[idx]\n",
    "    fp = os.path.split(fp)\n",
    "    name, extension = os.path.splitext(fp[-1])\n",
    "    \n",
    "    image, mask = train_dataset[idx]\n",
    "    \n",
    "    data = np.expand_dims(image, axis = 0)\n",
    "    data = torch.from_numpy(data)\n",
    "    pr_mask = model.predict(data.to('cuda'))\n",
    "    \n",
    "    act = nn.Sigmoid()\n",
    "    pr_mask = act(pr_mask)\n",
    "    pr_mask = pr_mask.cpu().numpy()\n",
    "    image = image.astype('uint8')\n",
    "    \n",
    "#     plt.figure(figsize=(6,6))\n",
    "#     plt.subplot(1,2,1).set_title(name)\n",
    "#     plt.imshow(image.transpose(1,2,0).reshape(1024, 1024,3), cmap = \"gray\")\n",
    "#     plt.imshow(pr_mask.squeeze().reshape(1024, 1024), alpha=0.5, cmap = cmap_no_background)\n",
    "#     plt.subplot(1,2,2)\n",
    "#     plt.imshow(mask.squeeze().reshape(1024, 1024), alpha=0.5, cmap = cmap_gt_background2)\n",
    "#     output_seg[name] = pr_mask.squeeze()\n",
    "    np.save(os.path.join(output_folder, name+'.npy'), pr_mask.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(valid_dataset)):\n",
    "    fp = valid_dataset.images_fps[idx]\n",
    "    fp = os.path.split(fp)\n",
    "    name, extension = os.path.splitext(fp[-1])\n",
    "    \n",
    "    image, mask = valid_dataset[idx]\n",
    "    \n",
    "    data = np.expand_dims(image, axis = 0)\n",
    "    data = torch.from_numpy(data)\n",
    "    pr_mask = model.predict(data.to('cuda'))\n",
    "    \n",
    "    act = nn.Sigmoid()\n",
    "    pr_mask = act(pr_mask)\n",
    "    pr_mask = pr_mask.cpu().numpy()\n",
    "    image = image.astype('uint8')\n",
    "    \n",
    "#     plt.figure(figsize=(6,6))\n",
    "#     plt.subplot(1,2,1).set_title(name)\n",
    "#     plt.imshow(image.transpose(1,2,0).reshape(1024, 1024,3), cmap = \"gray\")\n",
    "#     plt.imshow(pr_mask.squeeze().reshape(1024, 1024), alpha=0.5, cmap = cmap_no_background)\n",
    "#     plt.subplot(1,2,2)\n",
    "#     plt.imshow(mask.squeeze().reshape(1024, 1024), alpha=0.5, cmap = cmap_gt_background2)\n",
    "    \n",
    "    np.save(os.path.join(output_folder, name+'.npy'), pr_mask.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test for py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_dict = {}\n",
    "for test_image in sorted(os.listdir('/work/tsung1271232/ncku-dataset/train/dicom')):\n",
    "    dcm = pydicom.dcmread(os.path.join('/work/tsung1271232/ncku-dataset/train/dicom', test_image))\n",
    "    image = dcm.pixel_array\n",
    "    \n",
    "    image, mask = image_mask_preprocessing(image, np.zeros_like(image), 512, 512)\n",
    "    \n",
    "    data = image.transpose(2, 0, 1).astype('float32')\n",
    "    data = np.expand_dims(data, axis = 0)\n",
    "    \n",
    "    data = torch.from_numpy(data)\n",
    "    pr_mask = model.predict(data.to('cuda'))\n",
    "    act = nn.Sigmoid()\n",
    "    pr_mask = act(pr_mask)\n",
    "    pr_mask = pr_mask.cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(image.reshape(512, 512, 3), cmap = \"gray\")\n",
    "    plt.axis('off')\n",
    "    plt.savefig('./original.png', bbox_inches='tight') \n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(image.reshape(512, 512, 3), cmap = \"gray\")\n",
    "    plt.imshow(pr_mask.squeeze(), alpha=0.5, cmap= cmap_no_background, vmin=0, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "    plt.savefig('./predict_mask.png', bbox_inches='tight')\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
